import logging

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample

from data_preprocessing.utils import show_basic_data_statistics


class DataPreprocessorBuilder:
    def __init__(self, data_name='general', result_col_name='Result'):
        self.data = None
        self.logger = logging.getLogger(__name__)
        self.data_name = data_name
        self.result_col_name = result_col_name

    def load_data(self, file_path: str) -> 'DataPreprocessorBuilder':
        if not file_path.endswith('.csv'):
            raise ValueError("Only CSV files are supported.")

        if self.data is not None:
            self.__log_info("Data has already been loaded. It will be replaced.")

        self.data = pd.read_csv(file_path)
        self.__drop_duplicates()
        self.__log_info(f"Data shape: {self.data.shape}")
        self.__log_info("Data loaded successfully.")
        return self

    def __drop_duplicates(self):
        shape = self.data.shape
        self.data.drop_duplicates(inplace=True)
        self.__log_info(f"{shape[0] - self.data.shape[0]} duplicates have been removed.")
        self.__log_info("Duplicates have been removed.")
        return self

    def set_data(self, data: pd.DataFrame) -> 'DataPreprocessorBuilder':
        if self.data is not None:
            self.__log_info("Data has already been loaded. It will be replaced.")

        self.data = data
        self.__log_info("Data set successfully.")
        return self

    def show_basic_statistics(self, should_show: bool = False) -> 'DataPreprocessorBuilder':
        if not should_show:
            return self

        self.__require_data()

        show_basic_data_statistics(self.data, self.logger)

        return self

    def add_sum_of_permitted_permissions(self) -> 'DataPreprocessorBuilder':
        self.__require_data()

        self.data['SUM_OF_PERMITTED_PERMISSIONS'] = self.data.drop(self.result_col_name, axis=1).sum(axis=1)
        self.__log_info("Sum of permitted permissions has been added to the data.")

        return self

    def add_sum_of_columns(self, columns: list[str], new_column_name: str) -> 'DataPreprocessorBuilder':
        self.__require_data()

        self.data[new_column_name] = self.data[columns].sum(axis=1)
        self.__log_info(f"Sum of columns {columns} has been added to the data as '{new_column_name}'.")

        return self

    def normalize_label(self) -> 'DataPreprocessorBuilder':
        self.__require_data()

        le = LabelEncoder()
        self.data[self.result_col_name] = le.fit_transform(self.data[self.result_col_name])
        self.__log_info(f"Label column '{self.result_col_name}' has been normalized.")

        for i, label in enumerate(le.classes_):
            self.__log_info(f"Label '{label}' has been encoded as {i}.")

        return self

    def normalize_data(self) -> 'DataPreprocessorBuilder':
        self.__require_data()

        self.data = (self.data - self.data.min()) / (self.data.max() - self.data.min())
        self.__log_info("Data has been normalized.")

        return self

    def remove_unbalanced_columns(self, threshold=0.9) -> 'DataPreprocessorBuilder':
        self.__require_data()

        initial_column_count = self.data.shape[1]
        self.data = self.data.loc[:, (self.data.apply(
            lambda x: x.value_counts(normalize=True).iloc[0] if not x.empty else 1) < threshold)]
        removed_column_count = initial_column_count - self.data.shape[1]

        self.__log_info(f"Unbalanced columns have been removed. {removed_column_count} columns were removed.")
        self.__log_info(f"Data shape: {self.data.shape}")

        return self

    def cut_names_from_all_columns(self) -> 'DataPreprocessorBuilder':
        self.__require_data()

        for col in self.data.columns:
            if '.' in col:
                self.data[col.split('.')[-1]] = self.data[col]
                self.data.drop(col, axis=1, inplace=True)
        self.__log_info("Names have been cut from labels in all columns.")

        return self

    def filter_columns(self, columns: list[str]) -> 'DataPreprocessorBuilder':
        self.__require_data()

        for column in columns:
            if column not in self.data.columns:
                raise ValueError(f"Column '{column}' does not exist in the data.")

        self.data = self.data[columns]
        self.__log_info(f"{len(columns)} Columns have been filtered.")

        return self

    def remove_columns_containing_arrow(self) -> 'DataPreprocessorBuilder':
        self.__require_data()

        excluded_strings = ['android.permission', 'com.htc', 'com.google.android', 'com.android', 'BLUETOOTH',
                            'MODIFY_AUDIO_SETTINGS', 'DISABLE_KEYGUARD', self.result_col_name]

        counter = 0
        for column in self.data.columns:
            if all(excluded not in column for excluded in excluded_strings):
                self.data.drop(column, axis=1, inplace=True)
                counter += 1

        self.__log_info(f"{counter} columns containing '->' have been removed.")
        self.__log_info(f"Data shape: {self.data.shape}")

        return self

    def balance_classes(self, method='undersample') -> 'DataPreprocessorBuilder':
        self.__require_data()

        if method not in ['oversample', 'undersample']:
            raise ValueError("Method must be either 'oversample' or 'undersample'.")

        # Separate majority and minority classes
        majority_class = self.data[self.data[self.result_col_name] == self.data[self.result_col_name].mode()[0]]
        minority_class = self.data[self.data[self.result_col_name] != self.data[self.result_col_name].mode()[0]]

        self.__log_info(f"Amount of class 0.0: {majority_class.shape[0]}, amount of class 1.0: {minority_class.shape[0]}.")

        if method == 'oversample':
            # Oversample minority class
            minority_class_upsampled = resample(minority_class,
                                                replace=True,
                                                n_samples=majority_class.shape[0],
                                                random_state=42)
            self.data = pd.concat([majority_class, minority_class_upsampled])
        elif method == 'undersample':
            # Undersample majority class
            majority_class_downsampled = resample(majority_class,
                                                  replace=False,
                                                  n_samples=minority_class.shape[0],
                                                  random_state=42)
            self.data = pd.concat([majority_class_downsampled, minority_class])
            self.__log_info(
                f"After balancing amout of class 0.0: {majority_class_downsampled.shape[0]}, amount of class 1.0: {minority_class.shape[0]}.")

        else:
            raise ValueError("Method must be either 'oversample' or 'undersample'.")

        self.__log_info(f"Classes balanced using {method} method.")
        self.__log_info(f"Data shape: {self.data.shape}")

        return self

    def __require_data(self):
        if self.data is None:
            raise ValueError("Data has not been loaded yet.")

    def __log_info(self, message: str):
        self.logger.info(message, extra={'data_name': self.data_name})

    def build(self) -> pd.DataFrame:
        self.__require_data()
        return self.data
