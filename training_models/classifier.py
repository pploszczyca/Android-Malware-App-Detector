from abc import ABC
import time
from dataclasses import dataclass

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


@dataclass
class ModelStatistics:
    name: str
    accuracy: float
    confusion_matrix: any
    classification_report: dict
    prediction_time: dict[float, float]

    def __str__(self):
        return f"Classifier: {self.name}\n" \
               f"Accuracy: {self.accuracy}\n" \
               f"Confusion Matrix:\n{self.confusion_matrix}\n" \
               f"Classification Report:\n{self.classification_report}\n\n" \
               f"Time for prediction for {self.name} model:\n" \
               f"10% of data: {self.prediction_time[0.1]:.6f} seconds \n" \
               f"50% of data: {self.prediction_time[0.5]:.6f} seconds \n" \
               f"100% of data: {self.prediction_time[1.0]:.6f} seconds \n\n"


class Classifier(ABC):
    def __init__(self, training_data, test_data, name):
        self.name = name
        self.X_train = training_data.drop('Label', axis=1)
        self.y_train = training_data['Label']
        self.X_test = test_data.drop('Label', axis=1)
        self.y_test = test_data['Label']
        self.model = None
        self.grid_search = None

    def train(self):
        if self.grid_search is not None:
            self.grid_search.fit(self.X_train, self.y_train)
            self.model = self.grid_search.best_estimator_
            self.__print_best_params()
        else:
            self.model.fit(self.X_train, self.y_train)

    def __print_best_params(self):
        if self.grid_search:
            print(f"Best Parameters for {self.name}: {self.grid_search.best_params_}")

    def get_statistics(self) -> ModelStatistics:
        prediction_time = self.__test_and_get_time_of_prediction()

        return ModelStatistics(
            name=self.name,
            accuracy=self.__get_accuracy(self.X_test, self.y_test),
            confusion_matrix=self.__get_confusion_matrix(self.X_test, self.y_test),
            classification_report=self.__get_classification_report(self.X_test, self.y_test),
            prediction_time=prediction_time
        )

    def __get_accuracy(self, x_data, y_data):
        predictions = self.model.predict(x_data)
        return accuracy_score(y_data, predictions)

    def __get_confusion_matrix(self, x_data, y_data):
        predictions = self.model.predict(x_data)
        return confusion_matrix(y_data, predictions)

    def __get_classification_report(self, x_data, y_data) -> dict:
        predictions = self.model.predict(x_data)
        return classification_report(y_data, predictions, output_dict=True)

    def __test_and_get_time_of_prediction(self):
        sizes = [0.1, 0.5, 1.0]
        times = {}

        for size in sizes:
            start = time.perf_counter()
            X_sample_test = self.__get_sample(self.X_test, size)
            self.model.predict(X_sample_test)
            end = time.perf_counter()

            elapsed_time = end - start
            times[size] = elapsed_time

        return times

    @staticmethod
    def __get_sample(data, size):
        sample_size = int(len(data) * size)
        return data[:sample_size]

    def get_and_print_statistics(self) -> ModelStatistics:
        stats = self.get_statistics()
        print(stats)
        return stats
