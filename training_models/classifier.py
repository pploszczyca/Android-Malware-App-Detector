from abc import ABC
import time
from dataclasses import dataclass
from typing import Dict

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


@dataclass
class ModelStatistics:
    name: str
    accuracy: float
    confusion_matrix: any
    classification_report: Dict[str, Dict[str, float]]
    prediction_time: dict[float, float]

    def __str__(self):
        return f"Classifier: {self.name}\n" \
               f"Accuracy: {self.accuracy}\n" \
               f"Confusion Matrix:\n{self.confusion_matrix}\n" \
               f"Classification Report:\n{self.classification_report}\n\n" \
               f"Time for prediction for {self.name} model:\n" \
               f"10% of data: {self.prediction_time[0.1]:.6f} seconds \n" \
               f"50% of data: {self.prediction_time[0.5]:.6f} seconds \n" \
               f"100% of data: {self.prediction_time[1.0]:.6f} seconds \n\n"


class Classifier(ABC):
    def __init__(self, training_data, test_data, name):
        self.name = name
        self.X_train = training_data.drop('Label', axis=1)
        self.y_train = training_data['Label']
        self.X_test = test_data.drop('Label', axis=1)
        self.y_test = test_data['Label']
        self.model = None
        self.grid_search = None

    def train(self):
        if self.grid_search is not None:
            self.grid_search.fit(self.X_train, self.y_train)
            self.model = self.grid_search.best_estimator_
            self.__print_best_params()
        else:
            self.model.fit(self.X_train, self.y_train)

    def __print_best_params(self):
        if self.grid_search:
            print(f"Best Parameters for {self.name}: {self.grid_search.best_params_}")

    def get_statistics(self) -> ModelStatistics:
        prediction_time = self.__test_and_get_time_of_prediction()

        return ModelStatistics(
            name=self.name,
            accuracy=self.__get_accuracy(self.X_test, self.y_test),
            confusion_matrix=self.__get_confusion_matrix(self.X_test, self.y_test),
            classification_report=self.__get_classification_report(self.X_test, self.y_test),
            prediction_time=prediction_time
        )

    def __get_accuracy(self, x_data, y_data):
        predictions = self.model.predict(x_data)
        return accuracy_score(y_data, predictions)

    def __get_confusion_matrix(self, x_data, y_data):
        predictions = self.model.predict(x_data)
        return confusion_matrix(y_data, predictions)

    def __get_classification_report(self, x_data, y_data) -> dict:
        predictions = self.model.predict(x_data)
        return classification_report(y_data, predictions, output_dict=True)

    def __test_and_get_time_of_prediction(self):
        sizes = [0.1, 0.5, 1.0]
        number_of_tests = 10
        times = {}

        for size in sizes:
            elapsed_times = []
            X_sample_test = self.__get_sample(self.X_test, size)

            for _ in range(number_of_tests):
                start = time.perf_counter()
                self.model.predict(X_sample_test)
                end = time.perf_counter()

                elapsed_time = end - start
                elapsed_times.append(elapsed_time)

            average_time = sum(elapsed_times) / number_of_tests
            times[size] = average_time

        return times

    @staticmethod
    def __get_sample(data, size):
        sample_size = int(len(data) * size)
        return data[:sample_size]
