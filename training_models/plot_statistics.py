import os
from training_models.classifier import ModelStatistics
from typing import List
import matplotlib.pyplot as plt
import seaborn as sns


def plot_model_statistics(statistics: List[ModelStatistics], test_name: str):
    for plot in [plot_model_accuracies, plot_prediction_times, plot_classification_metrics, plot_confusion_matrices]:
        plot(statistics, test_name)


def ensure_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)


def plot_model_accuracies(statistics_list: List[ModelStatistics], test_name: str):
    # Extract model names and accuracies from the list of ModelStatistics objects
    model_names = [stat.name.replace('Classifier', '').strip() for stat in statistics_list]
    accuracies = [stat.accuracy for stat in statistics_list]

    # Create a bar plot
    plt.figure(figsize=(10, 6))
    bars = plt.bar(model_names, accuracies, color='skyblue')

    # Add title and labels
    plt.title(f'Model Accuracies - {test_name}')
    plt.xlabel('Model')
    plt.ylabel('Accuracy')

    # Add accuracy values on top of the bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')

    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, ha='right')

    # Adjust y-axis limits to better show differences
    plt.ylim(min(accuracies) - 0.05, max(accuracies) + 0.05)

    # Ensure directory exists
    dir_path = os.path.join('results', test_name)
    ensure_dir(dir_path)

    # Save plot to file
    plt.tight_layout()
    plt.savefig(os.path.join(dir_path, 'model_accuracies.png'))
    plt.show()


def plot_prediction_times(statistics_list: List[ModelStatistics], test_name: str):
    def create_bar_plot(data, labels, title, color, filename):
        plt.figure(figsize=(10, 6))
        bars = plt.bar(labels, data, color=color)
        plt.title(title)
        plt.xlabel('Model')
        plt.ylabel('Time (seconds)')
        for bar, value in zip(bars, data):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.6f}', ha='center', va='bottom')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()

        # Ensure directory exists
        dir_path = os.path.join('results', test_name)
        ensure_dir(dir_path)

        # Save plot to file
        plt.savefig(os.path.join(dir_path, filename))
        plt.show()

    model_names = [stat.name.replace('Classifier', '').strip() for stat in statistics_list]
    times_10 = [stat.prediction_time[0.1] for stat in statistics_list]
    times_50 = [stat.prediction_time[0.5] for stat in statistics_list]
    times_100 = [stat.prediction_time[1.0] for stat in statistics_list]

    # Plot for 10% data
    create_bar_plot(times_10, model_names, f'Prediction Time for 10% of Data - {test_name}', 'skyblue', 'prediction_time_10.png')

    # Plot for 50% data
    create_bar_plot(times_50, model_names, f'Prediction Time for 50% of Data - {test_name}', 'lightgreen', 'prediction_time_50.png')

    # Plot for 100% data
    create_bar_plot(times_100, model_names, f'Prediction Time for 100% of Data - {test_name}', 'salmon', 'prediction_time_100.png')


def plot_classification_metrics(statistics_list: List[ModelStatistics], test_name: str):
    metrics = ['precision', 'recall', 'f1-score', 'support']
    for metric in metrics:
        plt.figure(figsize=(12, 8))
        all_values = []
        for stat in statistics_list:
            data = [stat.classification_report[label][metric] for label in stat.classification_report if
                    isinstance(stat.classification_report[label], dict)]
            labels = [f"{stat.name.replace('Classifier', '').strip()} {label}" for label in stat.classification_report
                      if isinstance(stat.classification_report[label], dict)]
            plt.bar(labels, data, label=f"{stat.name.replace('Classifier', '').strip()}")
            all_values.extend(data)

        plt.title(f'{metric.capitalize()} of Models - {test_name}')
        plt.xlabel('Class')
        plt.ylabel(metric.capitalize())
        plt.legend()
        plt.xticks(rotation=45, ha='right')

        # Adjust y-axis limits to better show differences
        if metric != 'support':
            plt.ylim(min(all_values) - 0.05, max(all_values) + 0.05)
        else:
            plt.ylim(min(all_values) - 100, max(all_values) + 100)

        # Ensure directory exists
        dir_path = os.path.join('results', test_name)
        ensure_dir(dir_path)

        # Save plot to file
        plt.tight_layout()
        plt.savefig(os.path.join(dir_path, f'{metric}_of_models.png'))
        plt.show()


def plot_confusion_matrices(statistics_list: List[ModelStatistics], test_name: str):
    for stat in statistics_list:
        plt.figure(figsize=(8, 6))
        sns.heatmap(stat.confusion_matrix, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix for {stat.name.replace("Classifier", "").strip()} - {test_name}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')

        # Ensure directory exists
        dir_path = os.path.join('results', test_name)
        ensure_dir(dir_path)

        # Save plot to file
        plt.tight_layout()
        plt.savefig(os.path.join(dir_path, f'confusion_matrix_{stat.name.replace("Classifier", "").strip()}.png'))
        plt.show()
