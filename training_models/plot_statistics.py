from training_models.classifier import ModelStatistics

from typing import List
import matplotlib.pyplot as plt
import seaborn as sns


def plot_model_statistics(statistics: list[ModelStatistics]):
    for plot in [plot_model_accuracies, plot_prediction_times, plot_classification_metrics, plot_confusion_matrices]:
        plot(statistics)


def plot_model_accuracies(statistics_list: List[ModelStatistics]):
    # Extract model names and accuracies from the list of ModelStatistics objects
    model_names = [stat.name.replace('Classifier', '').strip() for stat in statistics_list]
    accuracies = [stat.accuracy for stat in statistics_list]

    # Create a bar plot
    plt.figure(figsize=(10, 6))
    bars = plt.bar(model_names, accuracies, color='skyblue')

    # Add title and labels
    plt.title('Model Accuracies')
    plt.xlabel('Model')
    plt.ylabel('Accuracy')

    # Add accuracy values on top of the bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')

    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, ha='right')

    # Adjust y-axis limits to better show differences
    plt.ylim(min(accuracies) - 0.05, max(accuracies) + 0.05)

    # Show plot
    plt.tight_layout()
    plt.show()


def plot_prediction_times(statistics_list: List[ModelStatistics]):
    model_names = [stat.name.replace('Classifier', '').strip() for stat in statistics_list]
    times_10 = [stat.prediction_time[0.1] for stat in statistics_list]
    times_50 = [stat.prediction_time[0.5] for stat in statistics_list]
    times_100 = [stat.prediction_time[1.0] for stat in statistics_list]

    # Create subplots
    fig, ax = plt.subplots(3, 1, figsize=(10, 18))

    # Plot for 10% data
    ax[0].bar(model_names, times_10, color='skyblue')
    ax[0].set_title('Prediction Time for 10% of Data')
    ax[0].set_xlabel('Model')
    ax[0].set_ylabel('Time (seconds)')
    for bar, time in zip(ax[0].patches, times_10):
        ax[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{time:.6f}', ha='center', va='bottom')

    # Plot for 50% data
    ax[1].bar(model_names, times_50, color='lightgreen')
    ax[1].set_title('Prediction Time for 50% of Data')
    ax[1].set_xlabel('Model')
    ax[1].set_ylabel('Time (seconds)')
    for bar, time in zip(ax[1].patches, times_50):
        ax[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{time:.6f}', ha='center', va='bottom')

    # Plot for 100% data
    ax[2].bar(model_names, times_100, color='salmon')
    ax[2].set_title('Prediction Time for 100% of Data')
    ax[2].set_xlabel('Model')
    ax[2].set_ylabel('Time (seconds)')
    for bar, time in zip(ax[2].patches, times_100):
        ax[2].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{time:.6f}', ha='center', va='bottom')

    # Adjust layout for better readability
    plt.tight_layout()
    plt.show()


def plot_classification_metrics(statistics_list: List[ModelStatistics]):
    metrics = ['precision', 'recall', 'f1-score']
    for metric in metrics:
        plt.figure(figsize=(12, 8))
        for stat in statistics_list:
            values = [stat.classification_report[label][metric] for label in stat.classification_report]
            labels = [f"{stat.name} {label}" for label in stat.classification_report]
            print(values)
            print(labels)
            plt.bar(labels, values, label=f'{stat.name} {metric}')

        plt.title(f'{metric.capitalize()} of Models')
        plt.xlabel('Class')
        plt.ylabel(metric.capitalize())
        plt.legend()
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()


def plot_confusion_matrices(statistics_list: List[ModelStatistics]):
    for stat in statistics_list:
        plt.figure(figsize=(8, 6))
        sns.heatmap(stat.confusion_matrix, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix for {stat.name}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()
